{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\anaconda3\\envs\\info_retrieve\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Resolving data files: 100%|██████████| 1500/1500 [00:00<00:00, 29670.10it/s]\n",
      "Found cached dataset imagefolder (C:/Users/Asus/.cache/huggingface/datasets/imagefolder/default-8f2e112e271d2f44/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "train_dataset = load_dataset('imagefolder',data_dir='./Landmark_Retrieval/Landmark_Retrieval/train/',split='train',drop_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=1500x924>,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {label:idx for idx,label in enumerate(train_dataset.features['label'].names)}\n",
    "id2label = {idx:label for idx,label in enumerate(train_dataset.features['label'].names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bao_tang_ha_noi': 0,\n",
       " 'buu_dien_trung_tam_tphcm': 1,\n",
       " 'cau_long_bien': 2,\n",
       " 'cau_nhat_tan': 3,\n",
       " 'cau_rong': 4,\n",
       " 'cho_ben_thanh_tphcm': 5,\n",
       " 'chua_cau': 6,\n",
       " 'chua_mot_cot': 7,\n",
       " 'chua_thien_mu': 8,\n",
       " 'cot_co': 9,\n",
       " 'hoang_thanh': 10,\n",
       " 'hon_chong_nhatrang': 11,\n",
       " 'landmark81': 12,\n",
       " 'lang_bac': 13,\n",
       " 'lang_khai_dinh': 14,\n",
       " 'mui_ca_mau': 15,\n",
       " 'mui_ke_ga_phanthiet': 16,\n",
       " 'nha_hat_lon_hanoi': 17,\n",
       " 'nha_hat_lon_tphcm': 18,\n",
       " 'nha_tho_da_co_sapa': 19,\n",
       " 'nha_tho_lon_ha_noi': 20,\n",
       " 'quang_truong_lam_vien': 21,\n",
       " 'suoi_tien_tphcm': 22,\n",
       " 'thac_ban_gioc': 23,\n",
       " 'thap_cham': 24,\n",
       " 'thap_rua': 25,\n",
       " 'toa_nha_bitexco_tphcm': 26,\n",
       " 'tuong_chua_kito_vungtau': 27,\n",
       " 'ubnd_tphcm': 28,\n",
       " 'van_mieu': 29}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "checkpoint = './results/checkpoint-500'\n",
    "# processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as numpy\n",
    "accuracy = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits,labels = eval_pred\n",
    "    predictions = numpy.argmax(logits,axis=1)\n",
    "    return accuracy.compute(predictions,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./results/checkpoint-500 were not used when initializing MobileNetV2Model: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing MobileNetV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MobileNetV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoModel\n",
    "# extractor = AutoFeatureExtractor.from_pretrained(checkpoint)\n",
    "model = AutoModel.from_pretrained(checkpoint).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import ToTensor, Compose,CenterCrop,Normalize\n",
    "pipe = Compose([CenterCrop(224),ToTensor(),Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))])\n",
    "def preprocess_function(examples):\n",
    "    examples['pixel_values']= [pipe(image.convert('RGB')) for image in examples['image']]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Asus\\.cache\\huggingface\\datasets\\imagefolder\\default-8f2e112e271d2f44\\0.0.0\\37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f\\cache-23b2b45667b70ba1.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(preprocess_function, batched=True,batch_size=32)\n",
    "train_dataset.set_format(type='torch', columns=['pixel_values', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_embeddings(examples):\n",
    "    # image = torch.stack(examples['pixel_values']).to('cuda')\n",
    "    with torch.no_grad():\n",
    "        image = examples['pixel_values'].to('cuda')\n",
    "        outputs = model(image)\n",
    "        embeddings = outputs.pooler_output\n",
    "    return {'embeddings':embeddings}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Asus\\.cache\\huggingface\\datasets\\imagefolder\\default-8f2e112e271d2f44\\0.0.0\\37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f\\cache-e06db96cb24f6e5a.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label', 'pixel_values', 'embeddings'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_dataset = train_dataset.select(range(2))\n",
    "temp_dataset = temp_dataset.map(get_embeddings,batched=True,batch_size=1)\n",
    "temp_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another = train_dataset.select(range(15))\n",
    "# temp_out= model(another['pixel_values'].to('cuda')).pooler_output\n",
    "# temp = {}\n",
    "# temp['embeddings'] = temp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(temp_dataset['pixel_values'].to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_dataset = train_dataset.select(range(1))\n",
    "# temp_dataset = temp_dataset.map(get_embeddings,batched=True,remove_columns=['image','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Asus\\.cache\\huggingface\\datasets\\imagefolder\\default-8f2e112e271d2f44\\0.0.0\\37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f\\cache-114ddb7d55286322.arrow\n"
     ]
    }
   ],
   "source": [
    "candidate_embeddings = train_dataset.map(get_embeddings,batched=True,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [00:15<00:00, 97.49it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "candidate_ids = []\n",
    "for id in tqdm(range(len(candidate_embeddings))):\n",
    "    label = candidate_embeddings[id]['label'].item()\n",
    "    entry = str(id) + '-' + str(id2label[label])\n",
    "    candidate_ids.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "all_embeddings = np.array(candidate_embeddings['embeddings'])\n",
    "all_embeddings = torch.from_numpy(all_embeddings).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sim(emb_one,emb_two):\n",
    "    return torch.cosine_similarity(emb_one,emb_two,dim=1).cpu().numpy().tolist()\n",
    "\n",
    "def fetch_sim(image,top_k=10):\n",
    "    # image = pipe(image.convert('RGB')).unsqueeze(0).to('cuda')\n",
    "    # img_pipe = Compose([CenterCrop(224),Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))])\n",
    "    image = image.unsqueeze(0).to('cuda')\n",
    "    new_batch = {'pixel_values':image}\n",
    "    with torch.no_grad():\n",
    "        new_batch = model(**new_batch).pooler_output\n",
    "    sim = compute_sim(new_batch,all_embeddings)\n",
    "    similarity_mapping = dict(zip(candidate_ids,sim))\n",
    "    sorted_similarity = dict(sorted(similarity_mapping.items(),key=lambda x:x[1],reverse=True))\n",
    "    id_entries = list(sorted_similarity.keys())[:top_k]\n",
    "    ids = list(map(lambda x:x.split('-')[0],id_entries))\n",
    "    labels = list(map(lambda x:x.split('-')[1],id_entries))\n",
    "    return ids,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 1500/1500 [00:00<00:00, 34065.32it/s]\n",
      "Found cached dataset imagefolder (C:/Users/Asus/.cache/huggingface/datasets/imagefolder/default-19797c74a360d3b4/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n",
      "Loading cached processed dataset at C:\\Users\\Asus\\.cache\\huggingface\\datasets\\imagefolder\\default-19797c74a360d3b4\\0.0.0\\37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f\\cache-94e8d5d89d4a6576.arrow\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_dataset('imagefolder',data_dir='./Landmark_Retrieval/Landmark_Retrieval/test/',split='train',drop_labels=False)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True,batch_size=32)\n",
    "test_dataset.set_format(type='torch', columns=['pixel_values', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1155\n",
      "thac_ban_gioc\n",
      "('1155', 'thac_ban_gioc')\n",
      "('1198', 'thac_ban_gioc')\n",
      "('1150', 'thac_ban_gioc')\n",
      "('1162', 'thac_ban_gioc')\n",
      "('1164', 'thac_ban_gioc')\n",
      "('1171', 'thac_ban_gioc')\n",
      "('1187', 'thac_ban_gioc')\n",
      "('1195', 'thac_ban_gioc')\n",
      "('1193', 'thac_ban_gioc')\n",
      "('1190', 'thac_ban_gioc')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# random.randint(0,len(test_dataset)\n",
    "image_num = random.randint(0,len(train_dataset))\n",
    "print(image_num)\n",
    "print(id2label[train_dataset[image_num].get('label').item()])\n",
    "test_sample = train_dataset[image_num]['pixel_values']\n",
    "# print(test_sample)\n",
    "sim = fetch_sim(test_sample)\n",
    "for i in zip(sim[0],sim[1]):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3242,  1.3242,  1.3242,  ...,  1.9235,  1.9578,  1.9749],\n",
       "         [ 1.3413,  1.3242,  1.2728,  ...,  1.9407,  1.9578,  1.9578],\n",
       "         [ 1.3070,  1.2385,  1.2557,  ...,  1.9235,  1.9407,  1.9578],\n",
       "         ...,\n",
       "         [-1.3302, -1.4329, -1.4672,  ..., -1.1932, -1.3302, -1.4500],\n",
       "         [-0.7993, -0.8507, -0.7308,  ..., -1.2445, -1.5699, -1.6384],\n",
       "         [-0.8849, -0.7650, -0.7137,  ..., -1.4158, -1.7240, -1.7069]],\n",
       "\n",
       "        [[ 1.8158,  1.8333,  1.8508,  ...,  2.2710,  2.2885,  2.3060],\n",
       "         [ 1.8508,  1.8508,  1.8508,  ...,  2.2710,  2.2885,  2.2885],\n",
       "         [ 1.9559,  1.8859,  1.9034,  ...,  2.2710,  2.2710,  2.2885],\n",
       "         ...,\n",
       "         [-0.6176, -0.7752, -0.9153,  ..., -0.4601, -0.6001, -0.7402],\n",
       "         [-0.1800, -0.2675, -0.2500,  ..., -0.5301, -0.8452, -0.9328],\n",
       "         [-0.3725, -0.2675, -0.2850,  ..., -0.7577, -1.0728, -1.0203]],\n",
       "\n",
       "        [[ 2.6400,  2.6400,  2.6051,  ...,  2.6400,  2.6400,  2.6400],\n",
       "         [ 2.6400,  2.5703,  2.4831,  ...,  2.6400,  2.6400,  2.6400],\n",
       "         [ 2.5877,  2.5180,  2.5006,  ...,  2.6400,  2.6400,  2.6400],\n",
       "         ...,\n",
       "         [-0.7761, -0.8110, -0.7064,  ..., -0.2707, -0.3753, -0.4798],\n",
       "         [ 0.1128,  0.0779,  0.2696,  ..., -0.3753, -0.6541, -0.6715],\n",
       "         [ 0.0953,  0.1476,  0.1825,  ..., -0.5844, -0.8633, -0.7587]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]['pixel_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['label', 'pixel_values'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 224), <f4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\info_retrieve\\lib\\site-packages\\PIL\\Image.py:3080\u001b[0m, in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   3079\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3080\u001b[0m     mode, rawmode \u001b[39m=\u001b[39m _fromarray_typemap[typekey]\n\u001b[0;32m   3081\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyError\u001b[0m: ((1, 1, 224), '<f4')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[1;32m----> 2\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mfromarray(test_dataset[\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mpixel_values\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mto(\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mnumpy())\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\info_retrieve\\lib\\site-packages\\PIL\\Image.py:3083\u001b[0m, in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   3081\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   3082\u001b[0m         msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCannot handle this data type: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m typekey\n\u001b[1;32m-> 3083\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m   3084\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3085\u001b[0m     rawmode \u001b[39m=\u001b[39m mode\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 224), <f4"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "image = Image.fromarray(test_dataset[0]['pixel_values'].to('cpu').numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info_retrieve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
